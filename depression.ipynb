{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOos4kGVBtibZfPI/fSBBtQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"KxpzyIfO_Yjg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698223209113,"user_tz":-60,"elapsed":43211,"user":{"displayName":"Chioma Ugbaja","userId":"03829113606355093072"}},"outputId":"33cff16b-e00c-497d-feb6-6961ce4a5727"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!cp drive/MyDrive/projects/ai-depression-detection/depression.py ."]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense\n","\n","import keras.optimizers\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import seaborn as sns # data visualization library\n","\n","import matplotlib.pyplot as plt\n","from pandas import DataFrame as df\n","\n","np.random.seed(42)\n","random_state = 42\n","\n","#df = pd.read_csv('DEPRESSION_PROJECT.csv')\n","\n","df = pd.read_csv('/content/drive/MyDrive/datasets/DEPRESSION_PROJECT.csv')\n","\n","\n","df.head()  # head method shows only first 5 rows\n","\n","# y includes our labels and x includes our features\n","y = df['CLASS']    # DEPRESSION or OTHERS\n","\n","X = df.drop(['CLASS'], axis = 1)\n","\n","#X.head()\n","\n","sns.countplot(df, x=\"CLASS\");\n","\n","D, O = y.value_counts()\n","print('Number of Depression: ',D)\n","print('Number of Others : ',O)\n","\n","\n","X.describe()\n","\n","x_=pd.get_dummies(X)\n","\n","#x_.to_csv('data.csv')\n","#X=pd.read_csv('data.csv')\n","\n","# for when running in google colab\n","x_.to_csv('/content/drive/MyDrive/datasets/data.csv')\n","X=pd.read_csv('/content/drive/MyDrive/datasets/data.csv')\n","\n","\n","X.shape\n","\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X_std = scaler.fit_transform(X)\n","\n","df=pd.DataFrame(X_std)\n","\n","df.hist(bins=50, figsize=(16, 10));\n","\n","X.shape\n","\n","#y\n","\n","\n","df.head(10)  # show the first 10 rows\n","\n","df\n","\n","from sklearn.preprocessing import LabelBinarizer\n","lb = LabelBinarizer()\n","y_=lb.fit_transform(y)\n","\n","\n","scaled_df = pd.concat([pd.DataFrame(X_std, columns=X.columns), y], axis=1)\n","\n","scaled_df.head(10)\n","\n","scaled_df.to_csv('Scaled.csv')\n","\n","from sklearn.decomposition import PCA\n","# feature extraction\n","pca = PCA(n_components=10)\n","X_pca = pca.fit_transform(X_std)\n","\n","\n","PCA_df = pd.DataFrame()\n","PCA_df['PCA_1'] = X_pca[:,0]\n","PCA_df['PCA_2'] = X_pca[:,1]\n","PCA_df['PCA_3'] = X_pca[:,2]\n","PCA_df.head(5)\n","\n","plt.figure(figsize=(8,6))\n","plt.plot(PCA_df['PCA_1'][scaled_df['CLASS'] == 'DEPRESSION'],PCA_df['PCA_2'][scaled_df['CLASS'] == 'DEPRESSION'],'o', alpha = 0.7, color = 'r')\n","plt.plot(PCA_df['PCA_1'][scaled_df['CLASS'] == 'OTHERS'],PCA_df['PCA_2'][scaled_df['CLASS'] == 'OTHERS'],'o', alpha = 0.7, color = 'b')\n","\n","plt.xlabel('PCA_1')\n","plt.ylabel('PCA_2')\n","plt.legend(['DEPRESSION','OTHERS'])\n","plt.show()\n","\n","# The amount of variance that each PC explains\n","var_exp = pca.explained_variance_ratio_\n","var_exp\n","\n","# Cumulative Variance explains\n","cum_var_exp = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4))\n","cum_var_exp\n","\n","plt.figure(figsize=(8,6))\n","plt.bar(range(1, len(pca.components_) + 1), var_exp, alpha=0.5, align='center', label='individual explained variance')\n","plt.step(range(1, len(pca.components_) + 1), cum_var_exp, where='mid', label='cumulative explained variance')\n","plt.plot(range(1, len(pca.components_) + 1), var_exp, 'ro-')\n","plt.xticks(range(1, len(pca.components_) + 1))\n","plt.ylabel('Explained Variance Ratio')\n","plt.xlabel('Principal Components')\n","plt.title('Scree Plot')\n","plt.legend(loc='best');\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score,confusion_matrix\n","\n","# split data train 50 % and test 50 %\n","#X_train, X_test, y_train, y_test = train_test_split(X_std, y_, test_size=0.1, random_state=random_state)\n","X_train, X_test, y_train, y_test = train_test_split(X_pca, y_, test_size=0.5, random_state=random_state)\n","\n","model = Sequential()\n","model.add(Dense(16, activation='relu', input_dim=10))\n","model.add(Dense(2, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","optimizer = keras.optimizers.legacy.SGD(lr=0.001,decay=0.0001, momentum=0.99)\n","\n","model.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n","\n","model.summary()\n","model.save('Depression.h5')\n","results = model.fit(X_train, y_train, epochs=130, batch_size=12,validation_split=0.3)\n","\n","#Training the model\n","scores = model.evaluate(X_train,y_train, verbose = 0)\n","print(print(\"%s,%.2f%%\" % ( model.metrics_names[1], scores[1] * 100)))\n","#Testing of the model\n","scores = model.evaluate(X_test,y_test, verbose = 0)\n","print(print(\"%s,%.2f%%\" % (model.metrics_names[1], scores[1] * 100)))\n","\n","\n","#correlelogram- plot heatmap to find correlation among features\n","corrmat =df.corr()\n","f, ax = plt.subplots(figsize=(30,20))\n","sns.heatmap(corrmat, square=True, annot=True,linewidth=0.6, cmap='RdBu')\n","\n","#Summarize history for accuracy\n","import matplotlib.pyplot as plt\n","plt.plot(results.history['accuracy'])\n","plt.title('tr_acc')\n","plt.ylabel('tr_acc')\n","plt.xlabel('tr_acc')\n","plt.legend(['tr_acc'], loc='upper left')\n","plt.show()\n","plt.plot(results.history['loss'])\n","plt.title('tr_loss')\n","plt.ylabel('tr_loss')\n","plt.xlabel('tr_epoch')\n","plt.legend(['tr_loss'], loc='upper left')\n","plt.show()\n","\n","plt.plot(results.history['val_accuracy'])\n","plt.title('val_accuracy')\n","plt.ylabel('val_accuracy')\n","plt.xlabel('val_accuracy')\n","plt.legend(['val_accuracy'], loc='upper left')\n","plt.show()\n","plt.plot(results.history['val_loss'])\n","plt.title('val_loss')\n","plt.ylabel('val_loss')\n","plt.xlabel('val_epoch')\n","plt.legend(['val_loss'], loc='upper left')\n","plt.show()\n","#model = load_model('depression.h5')\n","\n","plt.plot(results.history['accuracy'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","plt.plot(results.history['loss'])\n","plt.plot(results.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","plt.show()\n","\n","y_pred=model.predict(X_test)\n","y_pred=(y_pred>0.5)\n","y_pred.shape\n","y_test.shape\n","#y_pred=np.argmax(y_pred,axis=-1).reshape(-1,1)\n","\n","\n","\n","#Test Accuracy\n","\n","accu = accuracy_score(y_test,np.round(y_pred))\n","print('Accuracy is: ', accu)\n","\n","cm = confusion_matrix(y_test,y_pred)\n","#plt.figure(figsize=(3,3))\n","sns.heatmap(cm, annot=True, fmt=\"d\")\n","\n","from sklearn.metrics import f1_score\n","f1_score(y_pred, y_test)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ecTpDx9HqIZDV6RSkc98j7liqQhwO-g-"},"id":"pN_O6RdYBcGS","executionInfo":{"status":"ok","timestamp":1698226901288,"user_tz":-60,"elapsed":37717,"user":{"displayName":"Chioma Ugbaja","userId":"03829113606355093072"}},"outputId":"bcc994fd-e2cb-46ea-b858-55f49aed7cd8"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}